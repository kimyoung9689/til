### **이미지 생성 AI 기술의 발전 로드맵**

| 세대 | 연도 | 모델 | 핵심 개념 | 장점 | 단점 |
| --- | --- | --- | --- | --- | --- |
| **1세대**
(이미지 변환의 시작) | 2017 | **Pix2pix** | GANs 기반 짝 데이터 변환 | 흐릿한 이미지 문제를 해결하고 선명한 결과물을 만듦. | **짝을 이루는 데이터**가 반드시 필요함. |
|  | 2017 | **CycleGAN** | 사이클 일관성 | Pix2pix의 **짝 데이터 문제**를 해결함. | **형태 차이가 큰 객체** 변환에는 약함. |
| **2세대**
(이미지 변환의 확장) | 2019 | **InstaGAN** | 형태를 유지하지 않는 변환 | CycleGAN의 **형태 차이** 한계를 극복함. | 고해상도 이미지 생성에 한계가 있었음. |
|  | 2019 | **SPADE (GauGAN)** | 의미 분할 맵 활용 | 사용자가 그림을 그려 **다양한 스타일**을 만들 수 있게 함. | 입력이 의미 분할 맵으로 한정됨. |
|  | 2022 | **HyperStyle** | 사전 학습된 GAN의 잠재 공간 활용 | 모델의 **부분적 속성(나이, 헤어)을 효율적으로** 조작함. | 모델이 훈련된 도메인(예: 얼굴)에서만 작동함. |
| **3세대**
(텍스트 생성의 시작) | 2016 | **GAN-CLS** | 텍스트를 조건으로 이미지 생성 | 텍스트를 입력으로 받는 **최초의 모델** 중 하나. | **텍스트의 복잡한 정보**를 이해하지 못하고 저해상도 이미지를 만듦. |
|  | 2016 | **GAN-INT-CLS** | 보간법(Interpolation) | 텍스트의 유연한 이해로 **더 정확한 이미지**를 만듦. | 여전히 저해상도에 머물렀음. |
| **4세대**
(텍스트 생성의 완성) | 2023 | **GigaGAN** | 거대 규모 학습 | **텍스트 기반의 고해상도 이미지**를 만듦. | 대규모 컴퓨팅 자원이 필요함. |

---

### **이미지 생성 AI 기술의 최종 로드맵**

| 세대 | 연도 | 대표 모델 | 핵심 개념 | 장점 | 단점 |
| --- | --- | --- | --- | --- | --- |
| **1세대**
(GAN 기반 변환 시작) | 2017 | **Pix2pix, CycleGAN** | GANs 기반 이미지 변환 | **흐릿한 이미지 문제**와 **짝 데이터** 문제를 해결함. | **형태 차이** 변환에 약했음. |
| **2세대**
(GAN 기반 확장) | 2019~2022 | **InstaGAN, SPADE, HyperStyle** | 이미지 변환의 확장 | **형태 차이** 한계를 극복하고, **창작 자유도**와 **효율성**을 높임. | 여전히 GAN의 기본적 한계를 가짐. |
| **3세대**
(텍스트 생성 시작) | 2016 | **GAN-CLS, GAN-INT-CLS** | 텍스트를 조건으로 이미지 생성 | 텍스트를 이미지로 만드는 **새로운 시도**를 시작함. | **텍스트 이해가 부족**하고, **저해상도** 이미지를 생성함. |
| **4세대**
(텍스트 생성 완성) | 2023 | **GigaGAN** | 거대 규모 학습 | **고해상도** 텍스트 기반 이미지 생성을 현실로 만듦. | **대규모 컴퓨팅 자원**이 필요함. |
| **5세대**
(Diffusion 혁명) | 2021~2022 | **DALL·E, Stable Diffusion 등** | 노이즈 제거를 통한 이미지 생성 | **복잡한 프롬프트**를 정확히 이해하고, **매우 창의적인 고해상도** 이미지를 만듦. | **상대적으로 느린 생성 속도**와 **대규모 연산**이 필요함. |
| **6세대**
(통합 및 멀티모달) | 2023~현재 | **DALL·E 3, Flux 등** | 통합 및 향상된 이해 | **더 복잡한 프롬프트**를 정확하게 해석하고, **고품질 이미지**를 생성하며 다른 기술과 통합됨. | **높은 컴퓨팅 요구사항**과 **접근성 제한**이 있을 수 있음. |

---

### 확산 모델(Diffusion Model)

**확산** 현상을 활용한 모델
확산  : 어떤 물질이 시간이 지나면서 자연스럽게 섞여 퍼져나가는 현상

확산 모델은 이 원리를 거꾸로 이용하는 것

- **정방향 :** 원본 이미지에 노이즈를 조금씩 섞어 노이즈 덩어리로 만듦
- **역방향 :** 노이즈 덩어리에서 노이즈를 조금씩 제거,  거꾸로 원본 이미지를 만들어냄

확산 모델은 이런 과정을 통해 

새로운 이미지를 만들어내는 **생성 모델**의 한 종류

### 확산 모델의 핵심 구조

크게 두 가지 과정으로 이루어짐

정방향 확산 (Forward Diffusion)

이미지를 망가뜨리는 과정

**고정된 과정:** 이 과정은 딱히 학습할 필요가 없음. 

미리 정해둔 규칙에 따라 이미지를 망가뜨리면 됨

**노이즈 주입:** 단계(T)를 보통 1,000 정도로 설정, 매 단계마다 이미지에 작은 노이즈를 넣음. 시간이 지날수록 완전히 노이즈로 변함

역방향 확산 (Reverse Diffusion)

확산 모델의 진짜 능력. 노이즈 덩어리에서 이미지를 생성하는 과정

- **학습하는 과정:** 노이즈에서 이미지를 복원하는 건 어려우니,  신경망으로 학습
- **노이즈 제거:** 신경망이 노이즈를 얼마나 제거해야 원본에 가까워지는지 학습
- 모델은 노이즈만 잘 예측해서 빼주면 이미지 복원 가능

### 디노이징 확산 확률 모델 (DDPM)

2020년에 DDPM(Denoising Diffusion Probabilistic Model)이라는 모델 출시

기존 확산 모델의 손실 함수를 더 단순하게 만들어 이미지 생성 품질을 크게 높임

- **DDPM의 손실 함수:** 정방향 과정에서 얼마나 노이즈가 추가됐는지 예측하는 형태. 즉, "t시점에서 추가된 노이즈를 얼마나 잘 예측하는가?"가 핵심. 모델이 학습하기 쉬워짐
- **DDPM의 구조:** DDPM은 주로 **U-Net**이라는 신경망 구조를 사용해서 노이즈를 예측.   그리고 사인 곡선적 포지션 임베딩을 통해 현재 진행 단계(t) 정보를 모델에 알려줘서 노이즈를 더 정확하게 예측

---

### DDPM으로 이미지 생성하기

이미지 생성은 역방향 확산 과정과 같다.

1. **노이즈 준비:** 표준 정규 분포에서 완전히 무작위의 노이즈 덩어리를 가져오기
2. **노이즈 제거:** 노이즈 덩어리에서 모델이 예측한 노이즈를 조금씩 빼기
3. **반복:** 이 과정을 수천 번 반복, 노이즈가 점진적으로 제거되며 실제 이미지와 가까워짐

참고: 초기 단계(t가 클 때)에서는 이미지의 전체적인 형태나 핵심적인 특징 생성 

후기 단계(t가 작을 때)에서는 이미지의 세부적인 특징을 다듬기 

이렇게 단계적으로 이미지를 만들어내기 때문에 고해상도 이미지 생성가능해짐

---

### 확산 모델과 VAE의 차이

[OVAE는 이미지를 **하나의 잠재 변수**로 압축했다가 복원하는 방식

확산 모델은 이미지의 차원과 똑같은 크기의 **여러 단계의 잠재 변수**를 사용

VAE는 인코더와 디코더를 둘 다 학습

확산 모델은 정방향(인코더)은 미리 정해진 규칙대로만 움직이고 역방향(디코더)만 학습 

그래서 훨씬 안정적인 이미지 생성
