### **자연어처리(NLP)의 다양한 특수 분야들**

### 혐오 발언 탐지 (Hate Speech)

인터넷상에서 발생하는 혐오 발언이나 공격적인 표현을 자동으로 찾아내 분류하는 기술

**주요 과제:** 단순히 혐오 발언을 탐지를 넘어, 그에 대응하는 반박 대화 자동 생성 기술도 연구 중

- **데이터셋:**
    - **HateXplain:** 혐오 발언을 'Hate' 'Offensive' 'Normal'로 분류하고, 왜 그렇게 분류했는지 **설명 가능한 AI(XAI)** 기능이 포함된 데이터셋
    - **APEACH, Korean UnSmile Dataset, KOLD:** 한국어 혐오 발언 연구에 활용되는 주요 데이터셋

### 허위 정보 탐지 (Deception Detection)

가짜 뉴스, 팩트 체크, 신뢰성 평가 등 허위 정보를 탐지하는 기술

**Disinformation vs. Misinformation:** 의도적으로 허위 정보를 퍼뜨리는 **Disinformation**과 의도 없이 잘못된 정보를 퍼뜨리는 **Misinformation**을 구분하는 과제

**데이터셋:LIAR, FEVER:** 가짜 뉴스 탐지에 주로 사용되는 데이터셋으로, 텍스트가 사실인지 아닌지를 판별

### 기계 번역 (Machine Translation)

텍스트를 한 언어에서 다른 언어로 변환하는 기술

- **번역 품질 예측:** 번역된 결과물의 품질을 점수나 등급으로 자동으로 평가하는 거야.
- **자동 번역 수정 (Automatic Post Editing, APE):** 기계 번역 결과물을 사람이 조금만 수정해도 완벽한 번역물이 되도록 돕는 기술
- **채팅 번역 (Chat Translation):** 채팅 환경에 맞게 비격식적이고 자연스러운 번역을 제공하는 기술

### 대화 시스템 (Dialogue)

사용자와 자연스럽게 대화하는 시스템을 만드는 기술

- **페르소나 기반 대화:** 미리 정해진 성격(페르소나)을 가지고 대화하는 시스템
- **설득 대화:** 사용자를 설득하거나 행동을 유도하는 대화 시스템 생성
- **대화 요약:** 길고 복잡한 대화 내용을 핵심만 간결하게 요약하는 기술
- **지식 기반 대화:** 외부 지식을 활용하여 더 정확하고 풍부한 대화를 생성하는 기술

### 기타 NLP 분야

- **질문 생성:** 주어진 문서에서 내용을 이해하고 질문을 만드는 기술
- **문서 수준 관계 추출:** 여러 문장에 흩어져 있는 정보를 조합해 개체 간 복합적인 관계를 찾아내는 기술
- **LLM 평가:** LLM의 성능을 평가하는 방법론, 객관적인 벤치마크 점수 외에 사람이 직접 평가하는 정성적 평가의 중요성이 강조됨

### 한국어 관련 Task

- **고전어 번역:** 현대 한국어와 다른 고전 한국어를 현대어로 번역하는 기술
- **문법 교정 (GEC):** 한국어 문장의 문법적 오류를 자동으로 수정하는 기술
- **쓰기 평가:** 딥러닝 모델을 활용해 한국어 학습자의 쓰기 능력을 자동으로 평가하는 기술

---

### 규칙기반 및 통계기반 자연어처리 핵심 정리

## 1. 규칙 기반 NLP (Rule-Based NLP)

사람이 언어에 대한 **전문 지식**을 이용해서 직접 규칙을 만드는 방식

### 핵심 특징

- **규칙 기반 처리:** 사람이 만든 규칙에 딱 맞게 자연어를 처리하는 시스템
- **전문가의 지식:** 규칙을 만들려면 언어학적 지식 같은 Task에 대한 **전문 지식**이 꼭 필요
- **작동 원리 (자연어 처리 과정):** 문장이 들어오면 보통 아래 네 단계를 거침
    1. **형태소 분석:** 단어의 가장 작은 의미 단위(형태소)로 쪼개기
    2. **구문 분석:** 문장의 문법 구조(주어, 목적어 등)를 파악
    3. **의미 분석:** 문장의 뜻을 이해하는 단계
    4. **담화 분석:** 문장들이 모여서 말하는 사람의 의도(화행, Speech Act)까지 파악하는 단계

### 규칙 기반 NLP (Rule-Based NLP) 장단점

| 구분 | 장점 | 단점 |
| --- | --- | --- |
| **개발/처리** | 유연함 (Flexible)  | 숙련된 개발자 필요 (Requires skilled developers)  |
|  | 디버깅이 쉬움 (Easy to debug)  | 느린 구문 분석 (Slow parsing)  |
| **데이터/결과** | 많은 학습 데이터가 필요 없음 (Doesn't require much training)  | 보통의 커버리지 (Moderate coverage)  |
|  | 언어 이해 (Understanding language)  |  |
|  | 높은 정확도 (High precision) |  |

---

## 2. 통계 기반 NLP (Statistical NLP)

대량의 텍스트 데이터에서 **규칙을 직접 학습**하고 **통계**를 내서 단어를 표현하는 방식

### 핵심 특징

- **데이터 활용:** 위키피디아, 네이버, 구글 검색 같은 대량의 텍스트 데이터를 활용
- **통계 계산:** 특정 단어 주변에 어떤 단어가 얼마나 자주 등장하는지 세어서 단어의 의미를 파악
- **단어의 분산 표현:** 단어의 의미를 3차원 같은 벡터(숫자 목록)로 표현하는 게 목적
- **통계적 언어 모델 (SLM):** 이전 단어들을 보고 다음 단어가 나올 확률을 계산

### 한계점: 희소성 문제 (Sparsity Problem)

- 아무리 데이터가 많아도, 세상의 모든 문장을 다 담을 순 없다. 코퍼스(대규모 텍스트 모음)에
    
    **등장하지 않은 단어 순서**에 대해서는 다음 단어의 확률을 예측할 수가 없게 됨
    
- 이처럼 데이터 부족으로 언어를 정확하게 모델링하지 못하는 문제점을 희소성 문제라고 함

### 통계 기반 NLP (Statistical NLP) 장단점

| 구분 | 장점 (+) | 단점 (-) |
| --- | --- | --- |
| **개발/처리** | 확장 용이 (Easy to scale)  | 대량의 데이터 필요 (Requires large amount of data)  |
|  | 스스로 학습 (Learn by itself)  | 디버깅 어려움 (Difficult to debug)  |
|  | 빠른 개발 (Fast development)  | 문맥 이해 부족 (No understanding of context)  |
| **데이터/결과** | 높은 커버리지 (High coverage)  |  |
|  | 전문가의 중요성 감소 |  |

결론
자연어 처리의 역사는 

사람이 직접 가르치는 규칙에서 컴퓨터가 데이터로 스스로 배우는 통계/딥러닝으로 넘어왔다. 

규칙 기반은 정확하지만 손이 많이 가고, 

통계 기반은 대량의 데이터만 있다면 사람보다 빠르고 효율적으로 언어를 배울 수 있다. 

지금은 **통계 기반의 최종 진화형인 딥러닝**이 NLP를 이끌고 있다.

---

컴퓨터가 **스스로 학습**해서 언어를 처리하는 시대로 넘어옴

## 1. 머신러닝 기반 NLP

머신러닝(기계학습)은 컴퓨터가 데이터에서 **규칙을 스스로 찾아내도록** 가르치는 기술

### 작동 원리: 피처 엔지니어링 (Feature Engineering)

- **핵심:** 컴퓨터가 언어의 특징을 잘 알 수 있도록 사람이 특징을 직접 설계하고 추출해서 제공
- **예시:** 이메일에 광고 같은 단어가 5번 이상 나오면 스팸! 같이, 스팸 여부를 판단하는 특징을 사람이 직접 정해줌

### 주요 머신러닝 알고리즘

- **나이브 베이즈 (Naive Bayes):** 단어가 나타날 확률을 계산해서 분류하는 간단하고 빠른 방법
- **서포트 벡터 머신 (Support Vector Machine, SVM):** 데이터를 가장 잘 나누는 경계선을 찾아서 분류

---

## 2. 딥러닝 (Deep Learning) 기반 NLP

딥러닝은 머신러닝의 한 종류, 사람의 뇌를 모방한 **신경망** 구조를 여러 층으로 쌓아 올린 것

### 작동 원리: 피처를 스스로 학습

- **핵심:** 딥러닝은 사람의 도움 없이 데이터에서 **특징까지 스스로 학습**
- **장점:** 복잡하고 추상적인 특징(예: 단어의 미묘한 의미나 문맥)도 기계가 알아서 잡아내기 때문에, 사람이 일일이 특징을 설계할 필요가 없어서 성능이 훨씬 좋아짐

### 딥러닝의 핵심 기술: 단어 임베딩 (Word Embedding)

- **개념:** 단어의 의미를 숫자들의 목록(벡터)으로 표현하는 기술
- **특징:** 의미가 비슷한 단어(예: '사과'와 '바나나')는 벡터 공간에서 **가까운 위치**에 있게 됨
- **효과:** 컴퓨터가 '단어의 의미'와 '단어 간의 관계'를 이해할 수 있게 되면서 NLP 성능이 혁신적으로 개선 (예: Word2Vec, GloVe)

### 주요 딥러닝 모델 (현재 현업 주류)

- **RNN (순환 신경망) & LSTM (장단기 메모리):** 문장의 순서(시간 순서)대로 정보를 기억하면서 처리하는 모델. 특히 LSTM은 긴 문장의 정보를 잊어버리지 않도록 보완함
- **Transformer (트랜스포머):** **어텐션** 메커니즘을 사용해서 문장 전체의 단어들을 한 번에 보고 중요한 부분에 집중할 수 있게 만든 혁신적인 모델
- **BERT, GPT:** 트랜스포머를 기반으로 만들어진 모델들. 엄청나게 많은 데이터로 미리 학습시킨 후, 여러 NLP Task에 활용. 현재의 대규모 언어 모델(LLM)의 근간

---

## 3. 현업 코멘트 및 최종 결론

### 현업 코멘트

현재 NLP 현업은 **트랜스포머** 구조를 기반으로 한 **딥러닝** 모델이 지배중

- **머신러닝:** 복잡하지 않은 분류나 정해진 특징으로만 처리할 수 있는 간단한 Task에서 빠르고 가볍게 사용될 수 있지만, 최신 서비스에서는 거의 **딥러닝**으로 대체되는 추세
- **딥러닝 (LLM):** 챗봇, 자동 요약, 번역, 복잡한 질의응답 등 **거의 모든** 자연어 처리 분야에서 활용되며, 성능을 극한으로 끌어올리고 있다. 특히 미리 학습된 **LLM**을 가져와서 특정 Task에 맞게 조금만 학습하는 방식이 일반적임

### 최종 결론

자연어 처리는 **사람이 특징을 고르던 머신러닝** 시대를 넘어, **컴퓨터가 특징까지 스스로 학습하는 딥러닝 (트랜스포머 기반)** 시대로 접어들었고, 이 딥러닝 기술이 우리가 흔히 접하는 AI 서비스(챗GPT 등)의 핵심이 됨

---

### 뉴럴심볼릭 기반 자연언어처리

## 1. 뉴럴심볼릭 (Neural Symbolic)의 핵심

### 뉴럴심볼릭이란?

쉽게 말해 **'천재적인 감(딥러닝)'**과 **'논리적인 지식(심볼릭)'**을 합치는 거야.

1. **Neural (뉴럴):** 딥러닝(신경망) 모델이 가진 **뛰어난 감각** (데이터에서 패턴을 찾고 예측하는 능력).
2. **Symbolic (심볼릭):** 사람이 만든 **논리적인 규칙**이나 **지식 (지식 그래프)** (왜 그런지 설명할 수 있는 능력).

딥러닝 모델이 아무리 똑똑해도 "왜?"라고 물으면 대답을 못 하고, 가끔 엉뚱한 대답(추론 오류)을 하는 한계가 있거든. 뉴럴심볼릭은 이 한계를 극복하기 위해 **"외부의 상식 지식"**을 딥러닝에 주입해서

**추론 능력**과 **설명력**을 높이는 방법론이야.

## 2. 뉴럴심볼릭의 작동 원리 (지식 그래프)

심볼릭 지식은 주로

**지식 그래프(Knowledge Graph, KG)** 형태로 만들어져서 딥러닝 모델에 주입돼.

### 지식 그래프 (Knowledge Graph, KG)

- **구조:** 세상의 모든 지식을 **개체(Entity)**와 그 개체 사이의 **관계(Relation)**로 연결한 그래프 구조야. (예: `[사람] → [Likes] → [모나리자]`, `[모나리자] → [is in] → [루브르 박물관]`)
- **지식 주입:** 이 그래프 형태의 논리적 지식을 **Graph Neural Networks (GNN)** 같은 기술을 이용해서 딥러닝 모델이 이해할 수 있는 **숫자(벡터) 형태**로 변환하여 신경망에 내재화하는 거야.

다양한 지식 그래프 종류

- **백과사전형 KG:** 위키피디아 같은 사실 정보 (예: 오바마는 USA에 산다).
- **상식 KG (Commonsense KG):** 인간의 상식이나 배경지식 (예: 잠에서 깨는 것은 커피를 마시는 것으로 이어진다).
- **도메인별 KG:** 특정 분야의 전문 지식 (예: 의료, 법률).

---

## 3. 현업 코멘트 및 최종 결론

### 현업 코멘트

현재 대규모 언어 모델(LLM, 챗GPT 같은 모델)이 압도적인 성능을 보여주고 있지만, 여전히 **환각 현상(Hallucination)**, **오래된 지식** 또는 **추론 오류** 등의 문제가 있어.

- **뉴럴심볼릭의 역할:** 이 문제를 해결하기 위해 **LLM과 지식 그래프를 결합**하려는 연구가 활발해.
    - **KBQA (Knowledge Base Question Answering):** 지식 그래프를 활용해서 질문에 대한 정확한 답을 추론하고 , 단순한 질문뿐 아니라 여러 단계의 추론(Multi-hop QA)이 필요한 복잡한 질문에도 답을 찾게 해.
    - **KG-BERT:** 지식 그래프의 정보를 딥러닝 모델(BERT)이 더 잘 활용하도록 만들어 지식 기반을 완성하는 데 사용돼.
- **핵심 가치:** 이 접근 방식은 **정확성**과 **설명 가능성**이 절대적으로 필요한 **법률, 금융, 의료** 서비스 등 도메인 특화 분야에서 특히 중요하게 쓰이고 있다.
