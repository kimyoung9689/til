# PyTorch í˜„ì—… ì™„ë²½ ê°€ì´ë“œ

## 1. ê¸°ì´ˆ ê°œë… (í˜„ì—…ì—ì„œ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ê²ƒë“¤)

### 1.1 Tensor ê¸°ë³¸ - ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ ì¤‘ì‹¬
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# â­ í˜„ì—… í•µì‹¬: device ê´€ë¦¬ëŠ” ê¸°ë³¸ ì¤‘ì˜ ê¸°ë³¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Tensor ìƒì„± - í˜„ì—…ì—ì„œ ìì£¼ ì“°ëŠ” íŒ¨í„´ë“¤
x = torch.zeros(1000, 1000, device=device)  # GPUì—ì„œ ë°”ë¡œ ìƒì„±
x = torch.randn(32, 3, 224, 224, device=device)  # ë°°ì¹˜ ì´ë¯¸ì§€ í˜•íƒœ

# âš ï¸ ì ˆëŒ€ í”¼í•´ì•¼ í•  ì‹¤ìˆ˜: CPUâ†”GPU ì´ë™ ìµœì†Œí™”
# ë‚˜ìœ ì˜ˆ
bad_tensor = torch.randn(1000, 1000).to(device)  # CPUì—ì„œ ìƒì„± í›„ ì´ë™
# ì¢‹ì€ ì˜ˆ
good_tensor = torch.randn(1000, 1000, device=device)  # GPUì—ì„œ ë°”ë¡œ ìƒì„±
```

### 1.2 í˜„ì—… í•„ìˆ˜ Tensor ì¡°ì‘
```python
# ë°°ì¹˜ ì²˜ë¦¬ - í˜„ì—…ì˜ 99%ëŠ” ë°°ì¹˜ë¡œ ì²˜ë¦¬
batch_size = 32
seq_len = 128
hidden_dim = 768

# reshape vs view - ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ì´í•´ í•„ìˆ˜
x = torch.randn(batch_size, seq_len, hidden_dim)
# view: contiguousí•´ì•¼ í•¨ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
x_view = x.view(batch_size * seq_len, hidden_dim)
# reshape: í•­ìƒ ì‘ë™í•˜ì§€ë§Œ ë•Œë¡œ ë³µì‚¬ ë°œìƒ
x_reshape = x.reshape(batch_size * seq_len, hidden_dim)

# í˜„ì—…ì—ì„œ ìì£¼ ì“°ëŠ” indexing íŒ¨í„´ë“¤
indices = torch.tensor([0, 2, 4])
selected = x[indices]  # íŠ¹ì • ë°°ì¹˜ë§Œ ì„ íƒ

# Broadcasting - ë©”ëª¨ë¦¬ ì ˆì•½ì˜ í•µì‹¬
attention_mask = torch.ones(batch_size, 1, seq_len)  # (32, 1, 128)
scores = torch.randn(batch_size, seq_len, seq_len)   # (32, 128, 128)
masked_scores = scores * attention_mask  # broadcasting í™œìš©
```

## 2. ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„ (í˜„ì—… ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)

### 2.1 ëª¨ë“ˆ ì„¤ê³„ ì›ì¹™
```python
class ProductionModel(nn.Module):
    """í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ì„¤ê³„ íŒ¨í„´"""
    
    def __init__(self, config):
        super().__init__()
        # â­ config ê°ì²´ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê´€ë¦¬
        self.config = config
        
        # ë ˆì´ì–´ ì •ì˜ - ëª…í™•í•œ ë„¤ì´ë°
        self.embedding = nn.Embedding(config.vocab_size, config.hidden_dim)
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(config) for _ in range(config.num_layers)
        ])
        self.layer_norm = nn.LayerNorm(config.hidden_dim)
        self.classifier = nn.Linear(config.hidden_dim, config.num_classes)
        
        # â­ í˜„ì—… í•„ìˆ˜: weight ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Xavier/He ì´ˆê¸°í™” ë“± ì ìš©"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, input_ids, attention_mask=None):
        # â­ í˜„ì—…ì—ì„œëŠ” í•­ìƒ ë°°ì¹˜ ì²˜ë¦¬ ê°€ì •
        batch_size, seq_len = input_ids.shape
        
        # Embedding
        hidden_states = self.embedding(input_ids)
        
        # Encoder layers
        for layer in self.encoder_layers:
            hidden_states = layer(hidden_states, attention_mask)
        
        # Classification
        pooled = hidden_states.mean(dim=1)  # ê°„ë‹¨í•œ pooling
        logits = self.classifier(pooled)
        
        return logits
```

### 2.2 í˜„ì—…ì—ì„œ ìì£¼ ì“°ëŠ” ë ˆì´ì–´ë“¤
```python
class EncoderLayer(nn.Module):
    """Transformer ìŠ¤íƒ€ì¼ ì¸ì½”ë” ë ˆì´ì–´"""
    
    def __init__(self, config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.feed_forward = nn.Sequential(
            nn.Linear(config.hidden_dim, config.intermediate_dim),
            nn.GELU(),  # ReLUë³´ë‹¤ GELUê°€ í˜„ì—…ì—ì„œ ë§ì´ ì“°ì„
            nn.Dropout(config.dropout),
            nn.Linear(config.intermediate_dim, config.hidden_dim),
            nn.Dropout(config.dropout)
        )
        self.layer_norm1 = nn.LayerNorm(config.hidden_dim)
        self.layer_norm2 = nn.LayerNorm(config.hidden_dim)
    
    def forward(self, hidden_states, attention_mask=None):
        # â­ Residual connection + LayerNorm íŒ¨í„´
        # Self-attention
        attn_output = self.attention(hidden_states, attention_mask)
        hidden_states = self.layer_norm1(hidden_states + attn_output)
        
        # Feed-forward
        ff_output = self.feed_forward(hidden_states)
        hidden_states = self.layer_norm2(hidden_states + ff_output)
        
        return hidden_states

# CNNì—ì„œ ìì£¼ ì“°ëŠ” íŒ¨í„´
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)  # í˜„ì—… í•„ìˆ˜
        self.relu = nn.ReLU(inplace=True)  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))
```

## 3. í•™ìŠµ ê³¼ì • (í˜„ì—… ì¤‘ì‹¬)

### 3.1 ë°ì´í„° ë¡œë” - ì„±ëŠ¥ ìµœì í™”
```python
from torch.utils.data import DataLoader, Dataset
import torch.multiprocessing as mp

class ProductionDataset(Dataset):
    """í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ íŒ¨í„´"""
    
    def __init__(self, data_path, transform=None):
        self.data = self.load_data(data_path)
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        if self.transform:
            sample = self.transform(sample)
        return sample

# â­ í˜„ì—… ë°ì´í„°ë¡œë” ì„¤ì • - ì„±ëŠ¥ ìµœì í™”
def create_dataloader(dataset, batch_size, is_training=True):
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=is_training,
        num_workers=4,  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •
        pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        drop_last=is_training,  # ë§ˆì§€ë§‰ ë°°ì¹˜ í¬ê¸° ì¼ì •í•˜ê²Œ
        persistent_workers=True  # worker ì¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
    )
```

### 3.2 í•™ìŠµ ë£¨í”„ - í˜„ì—… í‘œì¤€
```python
class Trainer:
    """í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, model, optimizer, scheduler=None):
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.scaler = torch.cuda.amp.GradScaler()  # Mixed Precision
        
    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(dataloader):
            # GPUë¡œ ì´ë™
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            
            # â­ Mixed Precision Training (í˜„ì—… í•„ìˆ˜)
            with torch.cuda.amp.autocast():
                outputs = self.model(input_ids)
                loss = F.cross_entropy(outputs, labels)
            
            # Backward pass
            self.scaler.scale(loss).backward()
            
            # â­ Gradient Clipping (í˜„ì—…ì—ì„œ ìì£¼ ì‚¬ìš©)
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Optimizer step
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
            
            # Scheduler step (í•™ìŠµë¥  ì¡°ì •)
            if self.scheduler:
                self.scheduler.step()
            
            total_loss += loss.item()
            
            # â­ í˜„ì—…ì—ì„œëŠ” ì£¼ê¸°ì  ë¡œê¹… í•„ìˆ˜
            if batch_idx % 100 == 0:
                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        return total_loss / len(dataloader)
```

### 3.3 ìµœì í™” - í˜„ì—…ì—ì„œ ìì£¼ ì“°ëŠ” ì„¤ì •
```python
# Optimizer ì„¤ì • - í˜„ì—… ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
def create_optimizer(model, config):
    # AdamWê°€ í˜„ì—…ì—ì„œ ê°€ì¥ ë§ì´ ì“°ì„
    no_decay = ['bias', 'LayerNorm.weight']  # weight decay ì œì™¸í•  íŒŒë¼ë¯¸í„°
    
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.named_parameters() 
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': config.weight_decay,
        },
        {
            'params': [p for n, p in model.named_parameters() 
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
        },
    ]
    
    return torch.optim.AdamW(
        optimizer_grouped_parameters,
        lr=config.learning_rate,
        betas=(0.9, 0.999),
        eps=1e-8
    )

# Learning Rate Scheduler - í˜„ì—…ì—ì„œ ìì£¼ ì“°ëŠ” íŒ¨í„´
def create_scheduler(optimizer, num_training_steps, warmup_steps=None):
    if warmup_steps is None:
        warmup_steps = num_training_steps * 0.1  # ì „ì²´ì˜ 10%
    
    return torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=optimizer.param_groups[0]['lr'],
        total_steps=num_training_steps,
        pct_start=warmup_steps / num_training_steps
    )
```

## 4. ëª¨ë¸ ì €ì¥/ë¡œë“œ - í˜„ì—… í•„ìˆ˜ íŒ¨í„´

### 4.1 ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
```python
def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):
    """í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ì €ì¥ íŒ¨í„´"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'pytorch_version': torch.__version__,
    }
    torch.save(checkpoint, path)

def load_checkpoint(model, optimizer, scheduler, path):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    checkpoint = torch.load(path, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if scheduler and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    return checkpoint['epoch'], checkpoint['loss']

# â­ í˜„ì—…ì—ì„œëŠ” ëª¨ë¸ë§Œ ë”°ë¡œ ì €ì¥ë„ ë§ì´ í•¨
def save_model_only(model, path):
    """ì¶”ë¡ ìš© ëª¨ë¸ë§Œ ì €ì¥"""
    torch.save(model.state_dict(), path)

def load_model_for_inference(model_class, model_path, config):
    """ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ"""
    model = model_class(config)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model
```

## 5. í˜„ì—… ë””ë²„ê¹… & ëª¨ë‹ˆí„°ë§

### 5.1 ë©”ëª¨ë¦¬ ê´€ë¦¬ - í˜„ì—… í•„ìˆ˜
```python
import gc

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ - í˜„ì—…ì—ì„œ ìì£¼ ì‚¬ìš©"""
    torch.cuda.empty_cache()
    gc.collect()

def monitor_memory():
    """GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
    if torch.cuda.is_available():
        print(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"GPU Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

# Context managerë¡œ ë©”ëª¨ë¦¬ ì¶”ì 
class MemoryTracker:
    def __enter__(self):
        self.start_memory = torch.cuda.memory_allocated()
        return self
    
    def __exit__(self, *args):
        self.end_memory = torch.cuda.memory_allocated()
        print(f"Memory used: {(self.end_memory - self.start_memory) / 1024**2:.2f} MB")

# ì‚¬ìš© ì˜ˆ
with MemoryTracker():
    output = model(input_tensor)
```

### 5.2 ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```python
# â­ í˜„ì—…ì—ì„œ ëª¨ë¸ ì†ë„ ì¸¡ì •í•˜ëŠ” ë°©ë²•
def benchmark_model(model, input_tensor, num_runs=100):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    model.eval()
    
    # Warmup
    for _ in range(10):
        with torch.no_grad():
            _ = model(input_tensor)
    
    torch.cuda.synchronize()  # GPU ë™ê¸°í™” ì¤‘ìš”!
    start_time = time.time()
    
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(input_tensor)
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs
    print(f"Average inference time: {avg_time*1000:.2f} ms")
```

## 6. í˜„ì—… ë°°í¬ ì¤€ë¹„

### 6.1 ëª¨ë¸ ìµœì í™”
```python
# TorchScript ë³€í™˜ - í”„ë¡œë•ì…˜ ë°°í¬ìš©
def convert_to_torchscript(model, example_input):
    """TorchScriptë¡œ ë³€í™˜í•˜ì—¬ ë°°í¬ ìµœì í™”"""
    model.eval()
    
    # Trace ë°©ì‹
    traced_model = torch.jit.trace(model, example_input)
    
    # Script ë°©ì‹ (ì œì–´ íë¦„ì´ ìˆëŠ” ê²½ìš°)
    # scripted_model = torch.jit.script(model)
    
    return traced_model

# ì–‘ìí™” - ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
def quantize_model(model):
    """ë™ì  ì–‘ìí™”ë¡œ ëª¨ë¸ ìµœì í™”"""
    model.eval()
    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    return quantized_model
```

## 7. í˜„ì—…ì—ì„œ ì•Œì•„ë‘¬ì•¼ í•  í•„ìˆ˜ ì‚¬í•­ë“¤

### 7.1 ì„±ëŠ¥ ìµœì í™” íŒ
- **GPU ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë°°ì¹˜ í¬ê¸° ì¡°ì •, gradient checkpointing ì‚¬ìš©
- **Mixed Precision**: AMP ì‚¬ìš©ìœ¼ë¡œ 2ë°° ë¹ ë¥¸ í•™ìŠµ
- **DataLoader ìµœì í™”**: num_workers, pin_memory íŠœë‹
- **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**: Warmup + Cosine decay ì¡°í•©

### 7.2 ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸
- **NaN ì²´í¬**: `torch.isnan()` ì‚¬ìš©
- **Gradient í™•ì¸**: `torch.nn.utils.clip_grad_norm_()` í›„ norm ê°’ ë¡œê¹…
- **Shape ë¶ˆì¼ì¹˜**: `tensor.shape` ìì£¼ í™•ì¸
- **Device ë¶ˆì¼ì¹˜**: ëª¨ë“  tensorê°€ ê°™ì€ deviceì— ìˆëŠ”ì§€ í™•ì¸

### 7.3 í˜„ì—… ì½”ë”© ìŠ¤íƒ€ì¼
- **Config í´ë˜ìŠ¤**: ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ì•™ ê´€ë¦¬
- **ë¡œê¹…**: wandb, tensorboard ì ê·¹ í™œìš©
- **ì¬í˜„ì„±**: `torch.manual_seed()` ì„¤ì •
- **íƒ€ì… íŒíŠ¸**: í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì— íƒ€ì… ëª…ì‹œ

### 7.4 í˜‘ì—… ì‹œ ì£¼ì˜ì‚¬í•­
- **ëª¨ë¸ ë²„ì „ ê´€ë¦¬**: Git LFS ì‚¬ìš©
- **ì‹¤í—˜ ì¶”ì **: ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ ì²´ê³„ì  ê¸°ë¡
- **ì½”ë“œ ë¦¬ë·°**: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜, ì„±ëŠ¥ ì´ìŠˆ ì¤‘ì  ì²´í¬
- **ë¬¸ì„œí™”**: ëª¨ë¸ ì•„í‚¤í…ì²˜, í•™ìŠµ ê³¼ì • ìƒì„¸ ê¸°ë¡

---

**ğŸš€ í˜„ì—… ìƒì¡´ ê°€ì´ë“œ**

1. **ë©”ëª¨ë¦¬ê°€ ìƒëª…**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì€ í˜„ì—…ì—ì„œ ê°€ì¥ í”í•œ ë¬¸ì œ
2. **ë°°ì¹˜ ì²˜ë¦¬ ë§ˆìŠ¤í„°**: ëª¨ë“  ê²ƒì„ ë°°ì¹˜ë¡œ ìƒê°í•˜ë¼
3. **ì‹¤í—˜ ê¸°ë¡**: ë¬´ì—‡ì„ í–ˆëŠ”ì§€ ê¸°ë¡í•˜ì§€ ì•Šìœ¼ë©´ ì¬í˜„ ë¶ˆê°€
4. **ì„±ëŠ¥ ì¸¡ì •**: "ë¹ ë¥¸ ê²ƒ ê°™ë‹¤"ëŠ” ê¸ˆë¬¼, ì •í™•íˆ ì¸¡ì •í•˜ë¼
5. **ì ì§„ì  ê°œë°œ**: ì‘ì€ ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸í•˜ë©° ê°œë°œí•˜ë¼


