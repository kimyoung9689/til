# 딥러닝

## 1. 딥러닝 기초 개념

### 1.1 딥러닝이란

- 인공신경망을 기반으로 한 머신러닝의 한 분야
- 다층 신경망 구조를 통해 복잡한 패턴을 학습
- 인간의 뇌 신경망을 모방한 계산 모델

### 1.2 머신러닝 vs 딥러닝

- 머신러닝: 특징 추출을 수동으로 수행
- 딥러닝: 특징 추출을 자동으로 학습
- 데이터량이 많을수록 딥러닝이 우수한 성능

### 1.3 핵심 용어

- 뉴런(Neuron): 신경망의 가장 작은 계산 단위
- 가중치(Weight): **입력 신호의 중요성을 결정**하는 값 뉴런이 신호 받을때 뭐가 중요한지 숫자로 나타낸 것
- 편향(Bias): 뉴런이 신호를 내보낼지 말지 결정하는 **임계점**을 조정
- 순전파(Forward Propagation): 데이터가 신경망을 통과하는 과정. 입력에서 출력으로 계산
- 역전파(Backpropagation): 오차를 줄이기 위해 거꾸로 돌아가는 과정
- 순전파를 통해 나온 예측값이 실제 정답과 얼마나 다른지(오차)를 계산하고, 그 오차를 다시 거꾸로 되돌려 보내면서 가중치와 편향을 업데이트하는 과정

---

## 2. 신경망 구조

신경망 모델을 어떻게 쌓을지에 대한 설계도. 층(Layer)의 깊이와 뉴런의 연결 방식에 따라 모델의 학습 능력이 달라진다.

### 2.1 퍼셉트론

- 하나의 뉴런으로 이루어진 가장 단순한 신경망 모델
- 선형 분리 가능한 문제만 해결
- 수식: y = f(wx + b)

### 2.2 다층 퍼셉트론(MLP)

- 입력층, 은닉층, 출력층으로 구성
- 비선형 활성화 함수 사용
- XOR 문제와 같은 비선형 문제 해결 가능

### 2.3 심층 신경망

- 3개 이상의 은닉층을 가진 네트워크
- 계층적 특징 학습
- 표현 학습 능력 향상

---

## 3. 활성화 함수

뉴런의 계산 결과에 비선형성을 추가해 모델이 복잡한 문제를 풀 수 있게 돕는 역할.

### 3.1 시그모이드(Sigmoid)

- 함수: σ(x) = 1/(1+e^(-x))
- 출력 범위: (0, 1)
- 단점: 기울기 소실 문제
- 이진 분류의 출력층에서 최종 확률을 나타낼 때 거의 유일하게 쓰임(은닉층은 ReLU)

### 3.2 하이퍼볼릭 탄젠트(tanh)

- 함수: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
- 출력 범위: (-1, 1)
- 시그모이드보다 기울기 소실 문제 완화
- 예전에는 순환 신경망(RNN)에서 사용됐지만, 요즘은 ReLU나 그 외 다른 활성화 함수들이 더 좋은 성능을 보여주기 때문에 **거의 사용되지 않는 추세. LSTM 이나 GRU 주로사용 활성화함수도 ReLU계열 씀**

### 3.3 ReLU (Rectified Linear Unit)

- 함수: f(x) = max(0, x)
- 계산 효율적
- 기울기 소실 문제 해결
- 은닉층에서 주로 사용

### 3.4 기타 활성화 함수 (거의 은닉층)

- Leaky ReLU:  ReLU는 입력값이 0보다 작으면 출력이 0이 돼서 뉴런이 '죽는' 문제가 있었지. Leaky ReLU는 이 문제를 해결하기 위해 0보다 작은 값에도 아주 작은 기울기(0.01)를 줘. 덕분에 뉴런이 죽지 않고 학습을 이어갈 수 있음
- ELU: 출력값의 평균이 0에 가깝게 맞춰져서 학습 속도가 빨라지는 효과가 있음.ReLU보다 학습이 안정적
- Swish: 시그모이드 함수가 결합된 형태. ReLU보다 부드러운 곡선 형태를 띠어서 깊은 신경망에서 더 좋은 성능을 낼 수 있다고 알려져 있음
- GELU: 현재 **트랜스포머** 기반 최신 모델(GPT)에서 가장 많이 쓰이는 활성화 함수. 입력값에 확률적인 성질을 적용해서 학습 효율을 높임
- 소프트맥스 : 다중 분류의 출력층에서만 주로 쓰임. 여러 클래스의 확률을 나타낼 때 사용

### 3.5 선형 활성화

- 활성화 함수 없이, 입력값에 가중치를 곱하고 편향을 더한 값을 **그대로 출력**하는 방식
- 딥러닝 초보자들이 활성화 함수를 배울 때 잘 언급되지 않는 이유, **은닉층에서는 거의 사용하지 않기 때문**
- 선형 활성화 함수를 쓰면 아무리 층을 많이 쌓아도 결국 **하나의 직선**과 같아져서 복잡한 문제를 해결할 수 없다.
- **회귀 문제**의 **출력층에서 주로 쓰임. (숫자의 범위가 정해져 있지 않은 경우)**
- 정해져있으면 거의 시그모이드 사용 (회귀 문제에서 출력값 0과1)

---

## 4. 손실 함수

모델의 예측값이 정답과 얼마나 다른지 측정하는 역할

### 4.1 회귀 문제

- 평균 제곱 오차(MSE): 정답과 예측값의 **차이를 제곱**해서 평균 낸 값. 오차가 클수록 손실값이 늘어남
- 평균 절대 오차(MAE): 정답과 예측값의 **차이에 절댓값**을 씌운 뒤 평균 낸 값. 이상치 많을때 쓰기 좋음
- Huber Loss: MSE와 MAE의 결합. 오차가 작을 때는 MSE처럼, 오차가 클 때는 MAE처럼 선형적으로 변함

### 4.2 분류 문제

- 이진 교차 엔트로피: 이진분류할때 씀. 정답이 1일 때 예측 확률이 1에 , 정답이 0일 때 예측 확률이 0에 가까워지게 유도
- 범주형 교차 엔트로피: 다중분류에 사용. 정답 클래스에 해당하는 예측 확률(pi)을 높이도록 유도
- Sparse Categorical Crossentropy: 정답 레이블을 숫자로 입력. 원-핫 인코딩 생략하고 싶을때 사용

### 4.3 기타 손실 함수

- Focal Loss: 불균형 데이터용
- Dice Loss: 의료 영상처럼 픽셀 단위로 분류하는 문제(세그멘테이션)에 사용
- Triplet Loss: 비슷한 데이터끼리는 가깝게, 다른 데이터끼리는 멀게 만드는 임베딩 학습에 사용

### **주요 손실 함수 종류별 비교**

|  손실 함수 종류 |       문제 유형 |                           특징 |            대표적인 손실 함수 |
| --- | --- | --- | --- |
| **회귀 문제용** | **연속된 숫자 예측** | 정답과 예측값의 **수치적 차이**를 줄이는 데 초점을 맞춰. 오차를 제곱하거나 절댓값을 취해서 계산하지. | **평균 제곱 오차(MSE)**, **평균 절대 오차(MAE)**, Huber Loss |
| **분류 문제용** | **범주(클래스) 예측** | 정답 클래스의 **확률을 높이는** 방향으로 학습해. 예측 확률 분포와 정답 분포의 차이를 줄여. | **이진 교차 엔트로피**, **범주형 교차 엔트로피**, Sparse Categorical Cross-entropy |
| **기타 손실 함수** | **특수 목적** | 특정 상황(데이터 불균형, 이미지 분할 등)에서 일반적인 손실 함수보다 더 좋은 성능을 내기 위해 만들어졌어. | **Focal Loss**, Dice Loss, Triplet Loss |

250811
